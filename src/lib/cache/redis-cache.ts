/**
 * Cockpit - Redis Caching Layer
 *
 * High-performance caching with automatic invalidation
 * Expected performance: 90%+ query reduction, <5ms response time
 *
 * Features:
 * - Automatic cache invalidation
 * - Stale-while-revalidate pattern
 * - Type-safe cache keys
 * - TTL management
 * - Cache warming
 */

import { Redis } from "@upstash/redis";

// Initialize Redis client
const redis =
  process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN
    ? new Redis({
        url: process.env.UPSTASH_REDIS_REST_URL,
        token: process.env.UPSTASH_REDIS_REST_TOKEN,
      })
    : null;

/**
 * Cache configuration
 */
export const CACHE_CONFIG = {
  // L3 Catalog - 24 hours (rarely changes)
  L3_CATALOG_TTL: 86400, // 24 hours
  L3_CATALOG_STALE: 604800, // 7 days stale-while-revalidate

  // LOBs - 24 hours
  LOBS_TTL: 86400,

  // User data - 5 minutes
  USER_DATA_TTL: 300,

  // Dashboard data - 1 hour
  DASHBOARD_TTL: 3600,

  // Analytics - 15 minutes
  ANALYTICS_TTL: 900,

  // Session data - 30 minutes
  SESSION_TTL: 1800,
} as const;

/**
 * Type-safe cache keys
 */
export const CacheKeys = {
  // L3 Catalog
  l3CatalogAll: () => "l3:catalog:all",
  l3CatalogByLob: (lobName: string) => `l3:catalog:lob:${lobName}`,
  l3CatalogByModule: (module: string) => `l3:catalog:module:${module}`,
  l3CatalogByTier: (tier: string) => `l3:catalog:tier:${tier}`,
  l3CatalogSearch: (query: string) => `l3:catalog:search:${query}`,

  // LOBs
  lobsAll: () => "lobs:all",
  lobById: (id: string) => `lobs:id:${id}`,

  // User data
  userData: (userId: string) => `user:${userId}`,
  userProjects: (userId: string) => `user:${userId}:projects`,

  // Dashboard
  dashboardStats: (userId: string) => `dashboard:${userId}:stats`,
  dashboardCharts: (userId: string) => `dashboard:${userId}:charts`,

  // Analytics
  analyticsOverview: () => "analytics:overview",
  analyticsProject: (projectId: string) => `analytics:project:${projectId}`,
} as const;

/**
 * Cache statistics for monitoring
 */
export interface CacheStats {
  hits: number;
  misses: number;
  sets: number;
  deletes: number;
  errors: number;
  avgLatency: number;
}

class CacheStatsTracker {
  private stats: CacheStats = {
    hits: 0,
    misses: 0,
    sets: 0,
    deletes: 0,
    errors: 0,
    avgLatency: 0,
  };

  private latencies: number[] = [];

  recordHit(latency: number) {
    this.stats.hits++;
    this.recordLatency(latency);
  }

  recordMiss(latency: number) {
    this.stats.misses++;
    this.recordLatency(latency);
  }

  recordSet() {
    this.stats.sets++;
  }

  recordDelete() {
    this.stats.deletes++;
  }

  recordError() {
    this.stats.errors++;
  }

  private recordLatency(latency: number) {
    this.latencies.push(latency);
    if (this.latencies.length > 100) {
      this.latencies.shift();
    }
    this.stats.avgLatency = this.latencies.reduce((a, b) => a + b, 0) / this.latencies.length;
  }

  getStats(): CacheStats {
    return { ...this.stats };
  }

  reset() {
    this.stats = {
      hits: 0,
      misses: 0,
      sets: 0,
      deletes: 0,
      errors: 0,
      avgLatency: 0,
    };
    this.latencies = [];
  }
}

const statsTracker = new CacheStatsTracker();

/**
 * Redis cache wrapper with fallback to in-memory
 */
class CacheManager {
  private memoryCache: Map<string, { data: unknown; expires: number }> = new Map();
  private readonly useRedis: boolean;

  constructor() {
    this.useRedis = redis !== null;
    if (!this.useRedis) {
      console.warn("[Cache] Redis not configured, using in-memory fallback");
    }
  }

  /**
   * Get value from cache
   */
  async get<T>(key: string): Promise<T | null> {
    const startTime = performance.now();

    try {
      if (this.useRedis && redis) {
        const value = await redis.get<T>(key);
        const latency = performance.now() - startTime;

        if (value !== null) {
          statsTracker.recordHit(latency);
          console.log(`[Cache] ‚úÖ HIT ${key} (${latency.toFixed(2)}ms)`);
        } else {
          statsTracker.recordMiss(latency);
          console.log(`[Cache] ‚ùå MISS ${key} (${latency.toFixed(2)}ms)`);
        }

        return value;
      }

      // Memory fallback
      const cached = this.memoryCache.get(key);
      const latency = performance.now() - startTime;

      if (cached && cached.expires > Date.now()) {
        statsTracker.recordHit(latency);
        console.log(`[Cache] ‚úÖ HIT (memory) ${key}`);
        return cached.data as T;
      }

      statsTracker.recordMiss(latency);
      console.log(`[Cache] ‚ùå MISS (memory) ${key}`);
      return null;
    } catch (error) {
      statsTracker.recordError();
      console.error("[Cache] Error getting from cache:", error);
      return null;
    }
  }

  /**
   * Set value in cache with TTL
   */
  async set<T>(key: string, value: T, ttlSeconds: number): Promise<void> {
    try {
      if (this.useRedis && redis) {
        await redis.set(key, value, { ex: ttlSeconds });
        statsTracker.recordSet();
        console.log(`[Cache] üíæ SET ${key} (TTL: ${ttlSeconds}s)`);
      } else {
        // Memory fallback
        this.memoryCache.set(key, {
          data: value,
          expires: Date.now() + ttlSeconds * 1000,
        });
        statsTracker.recordSet();
        console.log(`[Cache] üíæ SET (memory) ${key} (TTL: ${ttlSeconds}s)`);
      }
    } catch (error) {
      statsTracker.recordError();
      console.error("[Cache] Error setting cache:", error);
    }
  }

  /**
   * Delete value from cache
   */
  async delete(key: string): Promise<void> {
    try {
      if (this.useRedis && redis) {
        await redis.del(key);
        statsTracker.recordDelete();
        console.log(`[Cache] üóëÔ∏è  DELETE ${key}`);
      } else {
        this.memoryCache.delete(key);
        statsTracker.recordDelete();
        console.log(`[Cache] üóëÔ∏è  DELETE (memory) ${key}`);
      }
    } catch (error) {
      statsTracker.recordError();
      console.error("[Cache] Error deleting from cache:", error);
    }
  }

  /**
   * Delete all keys matching pattern (Redis only)
   */
  async deletePattern(pattern: string): Promise<void> {
    try {
      if (this.useRedis && redis) {
        const keys = await redis.keys(pattern);
        if (keys.length > 0) {
          await redis.del(...keys);
          statsTracker.recordDelete();
          console.log(`[Cache] üóëÔ∏è  DELETE PATTERN ${pattern} (${keys.length} keys)`);
        }
      } else {
        // Memory fallback - delete matching keys
        const keysToDelete: string[] = [];
        for (const key of this.memoryCache.keys()) {
          if (this.matchPattern(key, pattern)) {
            keysToDelete.push(key);
          }
        }
        keysToDelete.forEach((key) => this.memoryCache.delete(key));
        console.log(`[Cache] üóëÔ∏è  DELETE PATTERN (memory) ${pattern} (${keysToDelete.length} keys)`);
      }
    } catch (error) {
      statsTracker.recordError();
      console.error("[Cache] Error deleting pattern from cache:", error);
    }
  }

  /**
   * Simple pattern matching for memory cache
   */
  private matchPattern(key: string, pattern: string): boolean {
    const regex = new RegExp("^" + pattern.replace(/\*/g, ".*") + "$");
    return regex.test(key);
  }

  /**
   * Get or set pattern - fetch from cache or compute and cache
   */
  async getOrSet<T>(key: string, fetcher: () => Promise<T>, ttlSeconds: number): Promise<T> {
    // Try cache first
    const cached = await this.get<T>(key);
    if (cached !== null) {
      return cached;
    }

    // Cache miss - fetch fresh data
    console.log(`[Cache] üîÑ FETCHING ${key}`);
    const data = await fetcher();

    // Cache the result
    await this.set(key, data, ttlSeconds);

    return data;
  }

  /**
   * Warm cache with frequently accessed data
   */
  async warmCache(key: string, fetcher: () => Promise<unknown>, ttlSeconds: number): Promise<void> {
    try {
      console.log(`[Cache] üî• WARMING ${key}`);
      const data = await fetcher();
      await this.set(key, data, ttlSeconds);
    } catch (error) {
      console.error(`[Cache] Error warming cache for ${key}:`, error);
    }
  }

  /**
   * Get cache statistics
   */
  getStats(): CacheStats {
    return statsTracker.getStats();
  }

  /**
   * Reset statistics
   */
  resetStats(): void {
    statsTracker.reset();
  }

  /**
   * Clear all cache (use with caution!)
   */
  async clearAll(): Promise<void> {
    try {
      if (this.useRedis && redis) {
        await redis.flushdb();
        console.log("[Cache] üóëÔ∏è  CLEARED ALL (Redis)");
      } else {
        this.memoryCache.clear();
        console.log("[Cache] üóëÔ∏è  CLEARED ALL (memory)");
      }
    } catch (error) {
      console.error("[Cache] Error clearing cache:", error);
    }
  }
}

/**
 * Singleton cache instance
 */
export const cache = new CacheManager();

/**
 * Cache helper for API routes
 */
export async function withCache<T>(
  key: string,
  fetcher: () => Promise<T>,
  ttlSeconds: number,
  options?: {
    forceRefresh?: boolean;
    staleWhileRevalidate?: boolean;
  }
): Promise<T> {
  if (options?.forceRefresh) {
    await cache.delete(key);
  }

  if (options?.staleWhileRevalidate) {
    // Return stale data immediately while refreshing in background
    const cached = await cache.get<T>(key);
    if (cached !== null) {
      // Refresh in background
      fetcher()
        .then((fresh) => cache.set(key, fresh, ttlSeconds))
        .catch((err) => console.error("[Cache] Background refresh failed:", err));

      return cached;
    }
  }

  return cache.getOrSet(key, fetcher, ttlSeconds);
}

/**
 * Invalidate related caches (for mutations)
 */
export async function invalidateCache(patterns: string[]): Promise<void> {
  for (const pattern of patterns) {
    await cache.deletePattern(pattern);
  }
}

/**
 * Export cache stats endpoint helper
 */
export function getCacheStats(): CacheStats {
  return cache.getStats();
}
